{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15c18aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageChops\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3ebdb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time, random\n",
    "from tqdm import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5e40cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 꺼내오기 \n",
    "# 독립문 훈련, 테스트셋 나누기 \n",
    "door_folder = \"C:/source/image/indep_door\"\n",
    "park_folder = \"C:/source/image/tapgol_park\"\n",
    "door_file_list = [x for x in os.listdir(door_folder)]\n",
    "park_file_list = [x for x in os.listdir(park_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5032c9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "door_test_list = door_file_list[:200]\n",
    "len(door_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e58097e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "door_train_list = door_file_list[200:]\n",
    "len(door_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "baa46c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 200/200 [00:14<00:00, 14.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# 독립문 테스트셋\n",
    "file_name_freq  = 0\n",
    "file_name_freq += 1 \n",
    "for i in tqdm(range(len(door_test_list))):\n",
    "    img = load_img(\"C:/source/image/indep_door/\" + door_test_list[i])\n",
    "    img = img.resize((300, 300))\n",
    "    img.save(\"C:/source/image/test/indep_door/\" + door_test_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "67797435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 136/136 [00:10<00:00, 12.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# 독립문 훈련셋\n",
    "for i in tqdm(range(len(door_train_list))):\n",
    "    img = load_img(\"C:/source/image/indep_door/\" + door_train_list[i])\n",
    "    img = img.resize((300, 300))\n",
    "    img.save(\"C:/source/image/train/indep_door/\" + door_train_list[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2194e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "park_test_list = park_file_list[:200]\n",
    "park_train_list = park_file_list[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d71f1b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 200/200 [00:19<00:00, 10.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# 탑골공원 테스트셋\n",
    "for i in tqdm(range(len(park_test_list))):\n",
    "    img = load_img(\"C:/source/image/tapgol_park/\" + park_test_list[i])\n",
    "    img = img.resize((300, 300))\n",
    "    img.save(\"C:/source/image/test/tapgol_park/\" + park_test_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1940520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 605/605 [00:57<00:00, 10.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# 탑골공원 훈련셋\n",
    "for i in tqdm(range(len(park_train_list))):\n",
    "    img = load_img(\"C:/source/image/tapgol_park/\" + park_train_list[i])\n",
    "    img = img.resize((300, 300))\n",
    "    img.save(\"C:/source/image/train/tapgol_park/\" + park_train_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ba04916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이미지 부풀리기\n",
    "datagen = ImageDataGenerator(\\\n",
    "                            rotation_range=10, # 정수범위만큼 무작위 회전\n",
    "                            width_shift_range=0.1,  # 픽셀단위로 좌우이동\n",
    "                            height_shift_range=0.1, # 픽셀단위로 상하이동\n",
    "                            shear_range=0.5, # 이미지 찌그러트리기\n",
    "                            zoom_range=0.2, # 이미지 확대 축소 \n",
    "                            horizontal_flip=True, # 좌우반전\n",
    "                            fill_mode=\"nearest\") # 이미지에 여백 생겼을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "003c3fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                     | 0/136 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/source/image/train/indep_door/HF010435_0103_0089.JPG'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-186-ab0a45e7b74c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfile_name_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoor_train_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/source/image/train/indep_door/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdoor_train_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# PIL 이미지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (3, 300, 300) 크기의 Numpy 배열\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (1, 3, 300, 300) 크기의 Numpy 배열\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    298\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msupported\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m   \"\"\"\n\u001b[1;32m--> 300\u001b[1;33m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0m\u001b[0;32m    301\u001b[0m                         target_size=target_size, interpolation=interpolation)\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[0;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'grayscale'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/source/image/train/indep_door/HF010435_0103_0089.JPG'"
     ]
    }
   ],
   "source": [
    "# 학습이미지만 부풀리기\n",
    "# 독립문 \n",
    "file_name_freq = 0\n",
    "for i in tqdm(range(len(door_train_list))):\n",
    "    img = load_img(\"C:/source/image/train/indep_door/\" + door_train_list[i])  # PIL 이미지\n",
    "    x = img_to_array(img) # (3, 300, 300) 크기의 Numpy 배열\n",
    "    x = x.reshape((1,) + x.shape) # (1, 3, 300, 300) 크기의 Numpy 배열\n",
    "    file_name_freq += 1\n",
    "    \n",
    "    # 변환된 이미지를 배치 단위로 생성해서 지정된 폴더에 저장\n",
    "    i = 0\n",
    "    for batch in datagen.flow(x, batch_size=1,\\\n",
    "                             save_to_dir=\"C:/source/image/train/indep_door/\",\\\n",
    "                             save_prefix=door_train_list[i] + str(file_name_freq),\\\n",
    "                             save_format=\"jpg\"):\n",
    "        i += 1\n",
    "        if i > 6:\n",
    "            break # 이미지 한 개 당 6개씩 부풀리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "16cff81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 학습이미지만 부풀리기\n",
    "# # 탑골공원 \n",
    "# file_name_freq = 0\n",
    "# for i in tqdm(range(len(park_train_list))):\n",
    "#     img = load_img(\"C:/source/image/train/tapgol_park/\" + park_train_list[i])  # PIL 이미지\n",
    "#     x = img_to_array(img) # (3, 300, 300) 크기의 Numpy 배열\n",
    "#     x = x.reshape((1,) + x.shape) # (1, 3, 300, 300) 크기의 Numpy 배열\n",
    "#     file_name_freq += 1\n",
    "    \n",
    "#     # 변환된 이미지를 배치 단위로 생성해서 지정된 폴더에 저장\n",
    "#     i = 0\n",
    "#     for batch in datagen.flow(x, batch_size=1,\\\n",
    "#                              save_to_dir=\"C:/source/image/train/tapgol_park/\",\\\n",
    "#                              save_prefix=park_train_list[i] + str(file_name_freq),\\\n",
    "#                              save_format=\"jpg\"):\n",
    "#         i += 1\n",
    "#         if i > 2:\n",
    "#             break # 이미지 한 개 당 6개씩 부풀리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c934170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 훈련시키기 \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers, initializers, regularizers, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3f25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11aa63bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1440 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory('D:/image/train/',target_size=(200,200),\n",
    "                                                    batch_size=b_size, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5797d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 480 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_directory('D:/image/validation/',target_size=(200,200),\n",
    "                                                    batch_size=b_size, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8dbb230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 480 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory('D:/image/test/',target_size=(200,200),\n",
    "                                                    batch_size=b_size, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "100cc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(20, (3, 3), input_shape=(200,200,3)))\n",
    "model.add(Activation(LeakyReLU(alpha=0.01)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(20, (3, 3)))\n",
    "model.add(Activation(LeakyReLU(alpha=0.01)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(40, (3, 3)))\n",
    "model.add(Activation(LeakyReLU(alpha=0.01)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(40))\n",
    "model.add(Activation(LeakyReLU(alpha=0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76836856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 198, 198, 20)      560       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 198, 198, 20)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 99, 99, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 97, 97, 20)        3620      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 97, 97, 20)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 48, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 46, 46, 40)        7240      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 46, 46, 40)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 23, 40)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 21160)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 40)                846440    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 82        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 857,942\n",
      "Trainable params: 857,942\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0280f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=0.0002),\\\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94602a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_train = len(train_generator)\n",
    "steps_test = len(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a1ca6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 288 steps, validate for 96 steps\n",
      "Epoch 1/20\n",
      "288/288 [==============================] - 6s 22ms/step - loss: 0.1550 - accuracy: 0.9361 - val_loss: 0.0268 - val_accuracy: 0.9958\n",
      "Epoch 2/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0365 - accuracy: 0.9882 - val_loss: 0.0190 - val_accuracy: 0.9958\n",
      "Epoch 3/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0168 - accuracy: 0.9958 - val_loss: 0.0146 - val_accuracy: 0.9958\n",
      "Epoch 4/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.0226 - val_accuracy: 0.9958\n",
      "Epoch 5/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0120 - accuracy: 0.9972 - val_loss: 0.0096 - val_accuracy: 0.9979\n",
      "Epoch 6/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0069 - val_accuracy: 0.9958\n",
      "Epoch 7/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0047 - accuracy: 0.9993 - val_loss: 0.0112 - val_accuracy: 0.9979\n",
      "Epoch 8/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.0038 - val_accuracy: 0.9958\n",
      "Epoch 10/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.0116 - val_accuracy: 0.9979\n",
      "Epoch 11/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.0060 - val_accuracy: 0.9979\n",
      "Epoch 12/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 0.9979\n",
      "Epoch 13/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 6.3609e-04 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 1.8591e-04 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9979\n",
      "Epoch 15/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 1.2209e-04 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 0.9979\n",
      "Epoch 16/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 5.3019e-04 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 0.9979\n",
      "Epoch 17/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0054 - val_accuracy: 0.9979\n",
      "Epoch 18/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0040 - accuracy: 0.9972 - val_loss: 0.0312 - val_accuracy: 0.9979\n",
      "Epoch 19/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0058 - accuracy: 0.9986 - val_loss: 0.0035 - val_accuracy: 0.9979\n",
      "Epoch 20/20\n",
      "288/288 [==============================] - 4s 15ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 0.0144 - val_accuracy: 0.9979\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "       train_generator,\n",
    "       steps_per_epoch=steps_train,\n",
    "       epochs=20,\n",
    "       validation_data=test_generator,\n",
    "       validation_steps=steps_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4811979",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Sequential()\n",
    "new_model.add(Conv2D(20, (3, 3), input_shape=(300,300,3)))\n",
    "model.add(Activation(LeakyReLU(alpha=0.01)))\n",
    "new_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "new_model.add(Conv2D(20, (3, 3)))\n",
    "model.add(Activation(LeakyReLU(alpha=0.01)))\n",
    "new_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "new_model.add(Conv2D(40, (3, 3)))\n",
    "model.add(Activation(LeakyReLU(alpha=0.01)))\n",
    "new_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(40))\n",
    "model.add(Activation(LeakyReLU(alpha=0.01)))\n",
    "new_model.add(Dropout(0.5))\n",
    "new_model.add(Dense(2))\n",
    "new_model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fb33962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000219E41AE588> and <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x00000219E4FBFFD0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000219E4220898> and <tensorflow.python.keras.layers.core.Flatten object at 0x00000219E4FAB780>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x00000218E38147B8> and <tensorflow.python.keras.layers.core.Activation object at 0x00000218E3842940>).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (49000, 40) and (21160, 40) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12784/333798691.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weight/jinsu_weight.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\project-env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name)\u001b[0m\n\u001b[0;32m    179\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[0;32m    180\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project-env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name)\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tf'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m       \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mby_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m         raise NotImplementedError(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project-env\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, save_path)\u001b[0m\n\u001b[0;32m   1268\u001b[0m         graph_view=self._graph_view)\n\u001b[0;32m   1269\u001b[0m     base.CheckpointPosition(\n\u001b[1;32m-> 1270\u001b[1;33m         checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\n\u001b[0m\u001b[0;32m   1271\u001b[0m     load_status = CheckpointLoadStatus(\n\u001b[0;32m   1272\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project-env\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, trackable)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mrestore_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore_from_checkpoint_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project-env\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_restore_from_checkpoint_position\u001b[1;34m(self, checkpoint_position)\u001b[0m\n\u001b[0;32m    906\u001b[0m     restore_ops.extend(\n\u001b[0;32m    907\u001b[0m         current_position.checkpoint.restore_saveables(\n\u001b[1;32m--> 908\u001b[1;33m             tensor_saveables, python_saveables))\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project-env\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mrestore_saveables\u001b[1;34m(self, tensor_saveables, python_saveables)\u001b[0m\n\u001b[0;32m    287\u001b[0m              \"expecting %s\") % (tensor_saveables.keys(), validated_names))\n\u001b[0;32m    288\u001b[0m       new_restore_ops = functional_saver.MultiDeviceSaver(\n\u001b[1;32m--> 289\u001b[1;33m           validated_saveables).restore(self.save_path_tensor)\n\u001b[0m\u001b[0;32m    290\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore_op\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project-env\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, file_prefix)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_single_device_savers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mrestore_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project-env\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, file_prefix)\u001b[0m\n\u001b[0;32m    100\u001b[0m                                           structured_restored_tensors):\n\u001b[0;32m    101\u001b[0m       restore_ops[saveable.name] = saveable.restore(\n\u001b[1;32m--> 102\u001b[1;33m           restored_tensors, restored_shapes=None)\n\u001b[0m\u001b[0;32m    103\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project-env\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\saveable_object_util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[0;32m    113\u001b[0m       \u001b[0mrestored_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrestored_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m       return resource_variable_ops.shape_safe_assign_variable_handle(\n\u001b[1;32m--> 115\u001b[1;33m           self.handle_op, self._var_shape, restored_tensor)\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project-env\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mshape_safe_assign_variable_handle\u001b[1;34m(handle, shape, value, name)\u001b[0m\n\u001b[0;32m    289\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[0mvalue_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m   \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m   return gen_resource_variable_ops.assign_variable_op(handle,\n\u001b[0;32m    293\u001b[0m                                                       \u001b[0mvalue_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project-env\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \"\"\"\n\u001b[0;32m   1114\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (49000, 40) and (21160, 40) are incompatible"
     ]
    }
   ],
   "source": [
    "new_model.load_weights('weight/jinsu_weight.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c4685f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=0.0002),\\\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d90c9c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 2s 17ms/step - loss: 37.5844 - accuracy: 0.6250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[37.58441987876631, 0.625]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac628f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
